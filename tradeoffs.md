# ğŸ§  Backend Architecture Decision Log â€” Aragon.ai Technical Challenge (Updated)

## Context
This log documents the design decisions and tradeoffs made while planning the **image upload & validation backend** for the Aragon.ai technical challenge.  
The primary goal was to deliver a **secure, scalable, production-aligned system** within a **3-hour implementation window**.  
Early discussions focused on upload strategies (direct backend vs presigned URLs), and later evolved to include storage selection, async processing, and concurrency handling.

---

## 1ï¸âƒ£ Initial Scope Discussion
**Faraz:**  
â€œWhat are the main backend mechanisms we actually need â€” just upload and validation?â€

**ChatGPT:**  
Outlined backend responsibilities:
- Upload & retrieval APIs  
- Validation pipeline  
- Metadata persistence (PostgreSQL + Prisma)  
- Optional async job system  
- Logging, error handling, and health checks  

**Outcome:**  
We established that core backend responsibilities are **upload**, **validation**, **metadata persistence**, and **asynchronous processing**.

---

## 2ï¸âƒ£ Upload Architecture
**Faraz:**  
â€œCan we upload directly from the UI to the storage (R2 or S3) using presigned URLs?â€

**ChatGPT:**  
Confirmed that **presigned direct-to-storage uploads** are production best practice.  
This offloads bandwidth from the backend, improves scalability, and allows independent validation.

**Outcome:**  
Adopted **presigned URL flow**:  
1. `POST /uploads/sign` â†’ returns presigned PUT URL + imageId.  
2. Client PUTs to storage (Cloudflare R2).  
3. Client calls `POST /uploads/complete` â†’ backend verifies and triggers validation.

---

## 3ï¸âƒ£ Tradeoff Analysis â€” Direct Upload vs Presigned
| Aspect | Presigned URL Flow | Direct Backend Upload |
|---------|--------------------|-----------------------|
| Scalability | âœ… Excellent â€” clientâ†’storage | âŒ Limited by backend bandwidth |
| Implementation Time | âš ï¸ Slightly longer | âœ… Quicker |
| Control & Validation | âœ… Requires verification step | âœ… Full control inline |
| Production Fit | âœ… Industry-standard | âš ï¸ MVP-only |

**Decision:**  
Use **presigned URLs** â€” slightly more setup, but shows production-grade architecture and scalability.

---

## 4ï¸âƒ£ Verification Strategy
**Faraz:**  
â€œWhat if the client says upload complete but the object isnâ€™t visible yet? Fail or retry?â€

**ChatGPT:**  
Explored options:  
1. Fail immediately â†’ fast but harsh UX.  
2. Retry verification â†’ best tradeoff.  
3. Event-driven (webhooks) â†’ ideal for long-term scalability.

**Decision:**  
âœ… **Option 2 â€“ Retry verification** with 3 attempts (5â€“15s delay).  
This tolerates eventual consistency on R2/S3 and improves reliability.

---

## 5ï¸âƒ£ Async Processing Decision
**Faraz:**  
â€œShould we use BullMQ, SQS, or something else for background validation?â€

**ChatGPT:**  
Critically compared options:
- **BullMQ (Redis)** â†’ easy to set up, persistent retries, good local dev.
- **AWS SQS** â†’ robust but heavy IAM setup and slower to integrate.
- **In-process queue** â†’ fast but no durability.

**Decision:**  
âœ… **BullMQ + Redis** â€” ideal balance of simplicity, reliability, and time-to-implement.  
SQS was dropped due to setup overhead and low visible benefit in an interview demo.

---

## 6ï¸âƒ£ State Machine & Concurrency
**Faraz:**  
â€œWhatâ€™s the lifecycle of an image? How do we handle two workers trying to update the same record?â€

**ChatGPT:**  
Designed a clear, auditable state machine:  
`AWAITING_UPLOAD â†’ VERIFYING â†’ PROCESSING â†’ ACCEPTED | REJECTED | UPLOAD_FAILED | ERROR`  

To prevent race conditions:
- **Deduplicate jobs** via `jobId=imageId` in BullMQ.  
- **Atomic conditional updates** (`WHERE id=imageId AND status=expected`) for safe state transitions.  
- **Idempotent worker logic** and transactional final writes.

**Outcome:**  
Guaranteed that each image is processed exactly once, safely retried if needed, and resilient to duplicate deliveries.

---

## 7ï¸âƒ£ Storage & Infra Tradeoffs
**Faraz:**  
â€œShould we use AWS S3 instead of Cloudflare R2?â€

**ChatGPT:**  
Compared both:
- **S3:** Better for deep AWS integrations (Rekognition, Lambda).  
- **R2:** Cheaper, simpler, S3-compatible, no egress cost, faster to set up.

**Decision:**  
âœ… **Cloudflare R2** â€” chosen for simplicity and alignment with project time constraints, while keeping S3 compatibility for future migration.

---

## 8ï¸âƒ£ Final Architecture Summary
- **Storage:** Cloudflare R2 (S3 API, presigned PUT URLs)  
- **Database:** PostgreSQL + Prisma (atomic transitions, strong consistency)  
- **Queue:** BullMQ + Redis (retry, delay, visibility)  
- **Concurrency Control:** DB-level conditional updates + job deduplication  
- **Workers:** Idempotent validation pipeline (safe re-runs)  
- **Validation:** Format, size, resolution, pHash, blur & face stubs  
- **UI:** React (converted from Aragon HTML) â€” mirrors original visual design  

---

## âœ… Final Outcome
This architecture is:
- **Scalable** â€” clientâ†’storage uploads, async workers  
- **Reliable** â€” verified uploads, retries, idempotent jobs  
- **Extensible** â€” can swap R2 â†’ S3 or BullMQ â†’ SQS later  
- **Realistic** â€” production patterns under interview constraints  
- **Demonstrative** â€” clearly shows backend craftsmanship and tradeoff reasoning

---
